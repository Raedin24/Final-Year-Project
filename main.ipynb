{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AlexNet',\n",
       " 'DenseNet',\n",
       " 'GoogLeNet',\n",
       " 'GoogLeNetOutputs',\n",
       " 'Inception3',\n",
       " 'InceptionOutputs',\n",
       " 'MNASNet',\n",
       " 'MobileNetV2',\n",
       " 'ResNet',\n",
       " 'ShuffleNetV2',\n",
       " 'SqueezeNet',\n",
       " 'VGG',\n",
       " '_GoogLeNetOutputs',\n",
       " '_InceptionOutputs',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_utils',\n",
       " 'alexnet',\n",
       " 'densenet',\n",
       " 'densenet121',\n",
       " 'densenet161',\n",
       " 'densenet169',\n",
       " 'densenet201',\n",
       " 'detection',\n",
       " 'googlenet',\n",
       " 'inception',\n",
       " 'inception_v3',\n",
       " 'mnasnet',\n",
       " 'mnasnet0_5',\n",
       " 'mnasnet0_75',\n",
       " 'mnasnet1_0',\n",
       " 'mnasnet1_3',\n",
       " 'mobilenet',\n",
       " 'mobilenet_v2',\n",
       " 'quantization',\n",
       " 'resnet',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnext101_32x8d',\n",
       " 'resnext50_32x4d',\n",
       " 'segmentation',\n",
       " 'shufflenet_v2_x0_5',\n",
       " 'shufflenet_v2_x1_0',\n",
       " 'shufflenet_v2_x1_5',\n",
       " 'shufflenet_v2_x2_0',\n",
       " 'shufflenetv2',\n",
       " 'squeezenet',\n",
       " 'squeezenet1_0',\n",
       " 'squeezenet1_1',\n",
       " 'utils',\n",
       " 'vgg',\n",
       " 'vgg11',\n",
       " 'vgg11_bn',\n",
       " 'vgg13',\n",
       " 'vgg13_bn',\n",
       " 'vgg16',\n",
       " 'vgg16_bn',\n",
       " 'vgg19',\n",
       " 'vgg19_bn',\n",
       " 'video',\n",
       " 'wide_resnet101_2',\n",
       " 'wide_resnet50_2']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "import sys\n",
    "import torch\n",
    "from torchvision import models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dir(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"../CSWin-Transformer/\")\n",
    "from models.cswin import CSWin_64_12211_tiny_224 as cswt, _conv_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CSWin model\n",
    "model = cswt(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Use the built-in __file__ attribute if it's defined, otherwise use the current working directory\n",
    "script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()\n",
    "\n",
    "# Construct the absolute path to the directory you want to add to the path\n",
    "path_to_add = os.path.join(script_dir, 'CSWin_Transformer')\n",
    "\n",
    "# Add the absolute path to sys.path\n",
    "sys.path.append(path_to_add)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained weights\n",
    "from conv_filter import _conv_filter\n",
    "\n",
    "\n",
    "pretrained_weights_path = 'Pretrained-Weight/cswin_tiny_224.pth'\n",
    "checkpoint = torch.load(pretrained_weights_path, map_location='gpu')\n",
    "state_dict = checkpoint['state_dict_ema']\n",
    "state_dict = _conv_filter(state_dict)  # Convert patch embedding weight to conv\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Load RAFDB using torchvision\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m imagenet_data \u001b[38;5;241m=\u001b[39m \u001b[43mtorchvision\u001b[49m\u001b[38;5;241m.\u001b[39mdatasets\u001b[38;5;241m.\u001b[39mImageNet(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRAF/DATASET\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(imagenet_data,\n\u001b[1;32m      4\u001b[0m                                           batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m,\n\u001b[1;32m      5\u001b[0m                                           shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m                                           num_workers\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mnThreads)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "#Load RAFDB using torchvision\n",
    "imagenet_data = torchvision.datasets.ImageNet('RAF/DATASET')\n",
    "data_loader = torch.utils.data.DataLoader(imagenet_data,\n",
    "                                          batch_size=4,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=args.nThreads)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CSWin_Transformer.models.cswin import CSWinTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mCSWinTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Final Year Project/FER_code/Working/Final-Year-Project/CSWin_Transformer/models/cswin.py:266\u001b[0m, in \u001b[0;36mCSWinTransformer.__init__\u001b[0;34m(self, img_size, patch_size, in_chans, num_classes, embed_dim, depth, split_size, num_heads, mlp_ratio, qkv_bias, qk_scale, drop_rate, attn_drop_rate, drop_path_rate, hybrid_backbone, norm_layer, use_chk)\u001b[0m\n\u001b[1;32m    264\u001b[0m curr_dim \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[1;32m    265\u001b[0m dpr \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, drop_path_rate, np\u001b[38;5;241m.\u001b[39msum(depth))]  \u001b[38;5;66;03m# stochastic depth decay rule\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m    267\u001b[0m     CSWinBlock(\n\u001b[1;32m    268\u001b[0m         dim\u001b[38;5;241m=\u001b[39mcurr_dim, num_heads\u001b[38;5;241m=\u001b[39mheads[\u001b[38;5;241m0\u001b[39m], reso\u001b[38;5;241m=\u001b[39mimg_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m, mlp_ratio\u001b[38;5;241m=\u001b[39mmlp_ratio,\n\u001b[1;32m    269\u001b[0m         qkv_bias\u001b[38;5;241m=\u001b[39mqkv_bias, qk_scale\u001b[38;5;241m=\u001b[39mqk_scale, split_size\u001b[38;5;241m=\u001b[39msplit_size[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    270\u001b[0m         drop\u001b[38;5;241m=\u001b[39mdrop_rate, attn_drop\u001b[38;5;241m=\u001b[39mattn_drop_rate,\n\u001b[1;32m    271\u001b[0m         drop_path\u001b[38;5;241m=\u001b[39mdpr[i], norm_layer\u001b[38;5;241m=\u001b[39mnorm_layer)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth[\u001b[38;5;241m0\u001b[39m])])\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge1 \u001b[38;5;241m=\u001b[39m Merge_Block(curr_dim, curr_dim\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    275\u001b[0m curr_dim \u001b[38;5;241m=\u001b[39m curr_dim\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/Final Year Project/FER_code/Working/Final-Year-Project/CSWin_Transformer/models/cswin.py:268\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    264\u001b[0m curr_dim \u001b[38;5;241m=\u001b[39m embed_dim\n\u001b[1;32m    265\u001b[0m dpr \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, drop_path_rate, np\u001b[38;5;241m.\u001b[39msum(depth))]  \u001b[38;5;66;03m# stochastic depth decay rule\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstage1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList([\n\u001b[1;32m    267\u001b[0m     CSWinBlock(\n\u001b[0;32m--> 268\u001b[0m         dim\u001b[38;5;241m=\u001b[39mcurr_dim, num_heads\u001b[38;5;241m=\u001b[39m\u001b[43mheads\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m, reso\u001b[38;5;241m=\u001b[39mimg_size\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m4\u001b[39m, mlp_ratio\u001b[38;5;241m=\u001b[39mmlp_ratio,\n\u001b[1;32m    269\u001b[0m         qkv_bias\u001b[38;5;241m=\u001b[39mqkv_bias, qk_scale\u001b[38;5;241m=\u001b[39mqk_scale, split_size\u001b[38;5;241m=\u001b[39msplit_size[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    270\u001b[0m         drop\u001b[38;5;241m=\u001b[39mdrop_rate, attn_drop\u001b[38;5;241m=\u001b[39mattn_drop_rate,\n\u001b[1;32m    271\u001b[0m         drop_path\u001b[38;5;241m=\u001b[39mdpr[i], norm_layer\u001b[38;5;241m=\u001b[39mnorm_layer)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth[\u001b[38;5;241m0\u001b[39m])])\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmerge1 \u001b[38;5;241m=\u001b[39m Merge_Block(curr_dim, curr_dim\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    275\u001b[0m curr_dim \u001b[38;5;241m=\u001b[39m curr_dim\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model = CSWinTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'CSWin_Transformer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mCSWin_Transformer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msegmentation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackbone\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcs_win_transformer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CSWin\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'CSWin_Transformer'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"CSWin-Transformer\")\n",
    "from models.cswin import CSWin_64_12211_tiny_224 as cswt, _conv_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CSWin model\n",
    "model = cswt(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['state_dict_ema'])\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained weights\n",
    "pretrained_weights_path = 'Pretrained-Weight/cswin_tiny_224.pth'\n",
    "checkpoint = torch.load(pretrained_weights_path, map_location='cpu')\n",
    "state_dict = checkpoint['state_dict_ema']\n",
    "state_dict = _conv_filter(state_dict)  # Convert patch embedding weight to conv\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "print(checkpoint.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
